{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D-nGnDmUD_GT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from pandas import DataFrame , concat\n",
        "from sklearn.metrics import mean_absolute_error , mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Activation\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.layers import LSTM\n",
        "from numpy import array , hstack\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "from warnings import simplefilter\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from keras_self_attention import SeqSelfAttention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-self-attention"
      ],
      "metadata": {
        "id": "G6PKlBjNKYqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1uk0I9THqEw",
        "outputId": "142aed19-83c9-4dab-cde9-952dc88a830b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Data/dataTS.csv', \n",
        "                   usecols = ['Time', 'pH', 'DO', 'TSS', 'TN', 'TP', 'TOC', 'ORP', 'Temp',\n",
        "       'TEMP'], index_col= 'Time', parse_dates=['Time']).reindex(columns=['pH', 'DO', 'TSS', 'TN', 'TP', 'TOC', 'ORP', 'Temp',\n",
        "       'TEMP'])\n",
        "# data[\"Date_time\"] = data.index.strftime('%H:%M:%S')\n",
        "# data[\"Month\"] = data.index.month\n",
        "# data[\"Day\"] = data.index.day\n",
        "# for colname in data.select_dtypes([\"object\", \"category\"]):\n",
        "#     data[colname], _ = data[colname].factorize()"
      ],
      "metadata": {
        "id": "5gk-Tc4RWgpq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_test = train_test_split(\n",
        "    data, test_size= 100, shuffle=False)\n",
        "data_train, data_val = train_test_split(\n",
        "    data_train, test_size= 100, shuffle=False)"
      ],
      "metadata": {
        "id": "muyoHNMCH3Rs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_input_output_pairs(data, time_steps, target_steps):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - time_steps - target_steps + 1):\n",
        "        X.append(data[i:i+time_steps,:])\n",
        "        y.append(data[i+time_steps:i+time_steps+target_steps,:])\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "97w9xiL2s5N_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = generate_input_output_pairs(data_train.values, time_steps=20, target_steps=10)\n",
        "X_val, y_val = generate_input_output_pairs(data_val.values, time_steps=20, target_steps=10)\n",
        "X_test, y_test = generate_input_output_pairs(data_test.values, time_steps=20, target_steps=10)\n",
        "# Add noise to data\n",
        "noise_factor = 0.05\n",
        "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)"
      ],
      "metadata": {
        "id": "3VBP2j_zlQ84"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of features and time steps\n",
        "num_features = 9\n",
        "time_steps = 20\n",
        "target_steps = 10\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Encoder LSTM\n",
        "    # tf.keras.layers.Normalization(input_shape=(time_steps, num_features)),\n",
        "    tf.keras.layers.LSTM(32, input_shape=(time_steps, num_features),activation='relu', return_sequences=False),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "    # tf.keras.layers.Dropout(0.2),\n",
        "    # tf.keras.layers.LSTM(1, activation='relu'),\n",
        "    tf.keras.layers.RepeatVector(target_steps),\n",
        "    SeqSelfAttention(attention_activation='softmax'),\n",
        "    \n",
        "    # Decoder LSTM\n",
        "    tf.keras.layers.LSTM(32, activation='relu', return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "    \n",
        "    # Output layer\n",
        "    tf.keras.layers.TimeDistributed(Dense(num_features))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "R_xlILXvEjdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0816da8e-8cb7-4e29-c5f3-536e06c684b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 32)                5376      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 10, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " seq_self_attention (SeqSelf  (None, 10, 32)           2113      \n",
            " Attention)                                                      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 10, 32)            8320      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 10, 64)            24832     \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 10, 9)            585       \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41,226\n",
            "Trainable params: 41,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model.fit([X_train_noisy], [y_train],\n",
        "                    validation_data=([X_val], [y_val]), \n",
        "                          batch_size=256, \n",
        "                          epochs=50, \n",
        "                          verbose=1, callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giKMXIGM8WoV",
        "outputId": "7ad3c460-8ef6-490b-be56-41c10e840e3c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1325/1325 [==============================] - 132s 94ms/step - loss: 22899.8789 - val_loss: 11740.9707\n",
            "Epoch 2/50\n",
            "1325/1325 [==============================] - 116s 87ms/step - loss: 17197.4219 - val_loss: 7250.2441\n",
            "Epoch 3/50\n",
            "1325/1325 [==============================] - 126s 95ms/step - loss: 13511.6406 - val_loss: 4421.4463\n",
            "Epoch 4/50\n",
            "1325/1325 [==============================] - 115s 87ms/step - loss: 10896.2236 - val_loss: 2678.8621\n",
            "Epoch 5/50\n",
            "1325/1325 [==============================] - 121s 91ms/step - loss: 9005.9951 - val_loss: 2228.6545\n",
            "Epoch 6/50\n",
            "1325/1325 [==============================] - 116s 88ms/step - loss: 7648.5654 - val_loss: 2156.1167\n",
            "Epoch 7/50\n",
            "1325/1325 [==============================] - 119s 90ms/step - loss: 6525.1924 - val_loss: 2044.3007\n",
            "Epoch 8/50\n",
            "1325/1325 [==============================] - 119s 90ms/step - loss: 5612.1748 - val_loss: 2026.8654\n",
            "Epoch 9/50\n",
            "1325/1325 [==============================] - 116s 88ms/step - loss: 5013.5020 - val_loss: 1941.0953\n",
            "Epoch 10/50\n",
            "1325/1325 [==============================] - 120s 90ms/step - loss: 4399.2656 - val_loss: 1949.5159\n",
            "Epoch 11/50\n",
            "1325/1325 [==============================] - 117s 88ms/step - loss: 3872.8225 - val_loss: 2172.8647\n",
            "Epoch 12/50\n",
            "1325/1325 [==============================] - 117s 89ms/step - loss: 3482.9214 - val_loss: 1993.4828\n",
            "Epoch 13/50\n",
            "1325/1325 [==============================] - 116s 88ms/step - loss: 3177.6782 - val_loss: 2222.0540\n",
            "Epoch 14/50\n",
            "1325/1325 [==============================] - 119s 90ms/step - loss: 3257.2969 - val_loss: 2285.3916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "np.save('lstm_autoencoder_attention.npy', y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xQVyOhEvOAn",
        "outputId": "8d6d5d41-8ceb-41e2-feed-ee20d882fdbe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 10ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in zip(range(9),['pH', 'DO', 'TSS', 'TN', 'TP', 'TOC', 'ORP', 'Temp', 'TEMP']):\n",
        "  print(j,' RMSE ',mean_squared_error(y_pred.reshape(-1,9)[:,i], y_test.reshape(-1,9)[:,i], squared = False))\n",
        "  print(j,' MAE ',mean_absolute_error(y_pred.reshape(-1,9)[:,i], y_test.reshape(-1,9)[:,i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijtp0Ti1X86x",
        "outputId": "6eefba33-83da-4dfb-ab36-fe24eb915579"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pH  RMSE  5.089031842117102\n",
            "pH  MAE  5.078244631109103\n",
            "DO  RMSE  1.945504490395827\n",
            "DO  MAE  1.9332233102019403\n",
            "TSS  RMSE  112.92961283195109\n",
            "TSS  MAE  91.0061176380585\n",
            "TN  RMSE  1.7323552653545509\n",
            "TN  MAE  1.4168979648004003\n",
            "TP  RMSE  58.913423054224594\n",
            "TP  MAE  44.23245770793829\n",
            "TOC  RMSE  9.366306772331729\n",
            "TOC  MAE  7.499297641294686\n",
            "ORP  RMSE  83.11849136242044\n",
            "ORP  MAE  79.23437255687445\n",
            "Temp  RMSE  2.3026838256149498\n",
            "Temp  MAE  2.185959792016258\n",
            "TEMP  RMSE  4.034697554160262\n",
            "TEMP  MAE  3.383624367576869\n"
          ]
        }
      ]
    }
  ]
}